import os
import argparse
import json
import base64
from datetime import datetime
from typing import List, Optional, Tuple
from dotenv import load_dotenv
from document_processor import extract_text_and_images_from_pdf  # ì—…ìŠ¤í…Œì´ì§€ Document Parser ì‚¬ìš©
from pymupdf_processor import extract_text_and_images_from_pdf as extract_with_pymupdf  # PyMuPDF ë°±ì—… ë°©ë²•
from model_handler import ModelHandler

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- Helper Functions ---

def get_image_mime_type(image_path: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì˜ MIME íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    ext = os.path.splitext(image_path)[1].lower()
    if ext == ".png":
        return "image/png"
    elif ext in [".jpg", ".jpeg"]:
        return "image/jpeg"
    return "application/octet-stream"

def image_to_base64(image_path: str) -> str:
    """ì´ë¯¸ì§€ íŒŒì¼ì„ base64 ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        mime_type = get_image_mime_type(image_path)
        return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}")
        return None


    """Base class for model handlers."""
    def __init__(self, model_type, model_name=None):
        self.model_type = model_type
        self.model_name = model_name





    """Handler for Upstage Solar API."""
    def __init__(self):
        if not UPSTAGE_API_KEY:
            raise ValueError("UPSTAGE_API_KEY is not set in .env file")
        
        self.headers = {
            "Authorization": f"Bearer {UPSTAGE_API_KEY}",
            "Content-Type": "application/json"
        }
    
    def generate_summary(self, text: str, image_paths: List[str] = None) -> str:
        """Summarize text using Upstage Solar API."""
        if image_paths:
            print(f"Note: Found {len(image_paths)} images. Images will be saved but not included in summarization for now.")
        
        prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ í•œêµ­ì–´ë¡œ ìì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. 
        - ì£¼ìš” í¬ì¸íŠ¸, í•µì‹¬ ì£¼ì¥, ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
        - ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        - ë¬¸ì„œì— ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ë³„ë„ë¡œ ì¶”ì¶œë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
        
        ë¬¸ì„œ ë‚´ìš©:
        {text}
        """

        payload = {
            "model": "solar-1-mini-chat",
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.3,
            "stream": False
        }

        try:
            response = requests.post(
                "https://api.upstage.ai/v1/chat/completions",
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



    """Handler for local Llama model using Ollama API."""
    def __init__(self, model_name: str = "llama3"):
        self.model_name = model_name
        self.api_url = "http://localhost:11434/api/generate"
        
        # Test if Ollama is running
        try:
            response = requests.get("http://localhost:11434/api/tags")
            if response.status_code != 200:
                raise ConnectionError("Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
        
        # Check if the model is available
        available_models = [m["name"] for m in response.json().get("models", [])]
        if self.model_name not in available_models:
            print(f"ê²½ê³ : ëª¨ë¸ '{self.model_name}'ì´(ê°€) ë¡œì»¬ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
            print(f"ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ollama pull {self.model_name}")
            self.model_name = available_models[0] if available_models else "llama3"
            print(f"ê¸°ë³¸ ëª¨ë¸ '{self.model_name}'ì„(ë¥¼) ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def generate_summary(self, text: str, image_paths: List[str] = None) -> str:
        """Summarize text using local Llama model via Ollama API."""
        if image_paths:
            print(f"Note: Found {len(image_paths)} images. Image analysis is not yet implemented for Llama.")
        
        # Simple prompt with Korean instruction
        prompt = f"""[INST] <<SYS>>
        ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        <</SYS>>
        
        ë‹¤ìŒ ë¬¸ì„œë¥¼ í•œêµ­ì–´ë¡œ ìì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” í¬ì¸íŠ¸, í•µì‹¬ ì£¼ì¥, ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {text}
        
        í•œêµ­ì–´ë¡œ ìš”ì•½: [/INST]"""
        
        try:
            response = requests.post(
                self.api_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "max_tokens": 1000,
                        "stop": ["</s>", "[INST]", "<|eot_id|>"]
                    }
                }
            )
            response.raise_for_status()
            return response.json().get("response", "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            return f"Ollama API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



    """Handler for OpenAI API."""
    def __init__(self, model_name: str = "text-davinci-003"):
        self.model_name = model_name
        self.api_url = "https://api.openai.com/v1/completions"
        self.headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
    
    def generate_summary(self, text: str, image_paths: List[str] = None) -> str:
        """Summarize text using OpenAI API."""
        if image_paths:
            print(f"Note: Found {len(image_paths)} images. Images will be saved but not included in summarization for now.")
        
        prompt = f"""Summarize the following text in Korean:
        
        {text}
        """
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "max_tokens": 1000,
            "temperature": 0.3,
            "stream": False
        }

        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["text"]
        except Exception as e:
            return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



    """Handler for Gemini API."""
    def __init__(self, model_name: str = "llama"):
        self.model_name = model_name
        self.api_url = "https://api.gemini.ai/v1/completions"
        self.headers = {
            "Authorization": f"Bearer {GEMINI_API_KEY}",
            "Content-Type": "application/json"
        }
    
    def generate_summary(self, text: str, image_paths: List[str] = None) -> str:
        """Summarize text using Gemini API."""
        if image_paths:
            print(f"Note: Found {len(image_paths)} images. Images will be saved but not included in summarization for now.")
        
        prompt = f"""Summarize the following text in Korean:
        
        {text}
        """
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "max_tokens": 1000,
            "temperature": 0.3,
            "stream": False
        }

        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload
            )
            response.raise_for_status()
            return response.json()["choices"][0]["text"]
        except Exception as e:
            return f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# --- Main Execution ---

# extract_text_and_images_from_pdf í•¨ìˆ˜ëŠ” document_processor.pyì—ì„œ ê°€ì ¸ì˜´
# ì—…ìŠ¤í…Œì´ì§€ Document Parser APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œ

def process_pdf(pdf_path: str, model_type: str, output_dir: str = "output", model_name: Optional[str] = None, parser: str = "auto", language: str = "ko") -> str:
    """Process a PDF file and generate a summary using the specified model.
    
    Args:
        pdf_path: Path to the PDF file
        model_type: Type of model to use ('upstage', 'llama', 'openai', 'gemini')
        output_dir: Directory to save output files
        model_name: Specific model name/version to use
        
    Returns:
        Generated summary text
    """
    # Create output directory with timestamp and model name
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
    pdf_specific_output_dir = os.path.join(output_dir, f"{pdf_name}_{model_type.upper()}_{timestamp}")
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(pdf_specific_output_dir, exist_ok=True)
    log_file = os.path.join(pdf_specific_output_dir, "process_log.txt")
    
    # ë¡œê·¸ í•¨ìˆ˜ ì •ì˜
    def log_message(message: str):
        print(message)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {message}\n")
    
    try:
        # ë°˜ë“œì‹œ ModelHandlerë§Œ ì‚¬ìš©í•˜ë„ë¡ ê³ ì •
        handler = ModelHandler(model_type, model_name, language)
        
        # Extract text and images from PDF
        log_message(f"PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(pdf_path)}")
        log_message(f"ëª¨ë¸: {model_type.upper()}{' - ' + model_name if model_name else ''}")
        log_message(f"ì–¸ì–´: {language}")
        log_message(f"íŒŒì„œ ëª¨ë“œ: {parser}")
        
        parser_used = ""  # ì‹¤ì œ ì‚¬ìš©ëœ íŒŒì„œ ì •ë³´ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        
        # íŒŒì„œ ì„ íƒ ë¡œì§
        if parser == "pymupdf" or parser == "local":
            # PyMuPDF ì§ì ‘ ì‚¬ìš©
            log_message("PyMuPDF íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬ ì¤‘...")
            text, image_paths = extract_with_pymupdf(pdf_path, pdf_specific_output_dir)
            parser_used = "PyMuPDF"  # íŒŒì„œ ì •ë³´ ì—…ë°ì´íŠ¸
        elif parser == "upstage" or parser == "api":
            # ì—…ìŠ¤í…Œì´ì§€ API ì§ì ‘ ì‚¬ìš©
            log_message("ì—…ìŠ¤í…Œì´ì§€ Document Parser APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬ ì¤‘...")
            try:
                text, image_paths = extract_text_and_images_from_pdf(pdf_path, pdf_specific_output_dir)
                parser_used = "Upstage Document Parser API"  # íŒŒì„œ ì •ë³´ ì—…ë°ì´íŠ¸
                if not text.strip():
                    log_message("âš ï¸ ê²½ê³ : ì—…ìŠ¤í…Œì´ì§€ APIì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    raise ValueError("ì—…ìŠ¤í…Œì´ì§€ APIì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                log_message(f"âŒ ì˜¤ë¥˜: ì—…ìŠ¤í…Œì´ì§€ API ì‹¤íŒ¨: {str(e)}")
                raise e  # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìë™ ì „í™˜ ì—†ì´ ì¢…ë£Œ
        else:  # auto ëª¨ë“œ (ê¸°ë³¸ê°’)
            # ë¨¼ì € ì—…ìŠ¤í…Œì´ì§€ API ì‹œë„, ì‹¤íŒ¨ ì‹œ PyMuPDFë¡œ ìë™ ì „í™˜
            log_message("ì—…ìŠ¤í…Œì´ì§€ Document Parser APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ì²˜ë¦¬ ì¤‘... (ì‹¤íŒ¨ ì‹œ PyMuPDFë¡œ ìë™ ì „í™˜)")
            try:
                # ì—…ìŠ¤í…Œì´ì§€ Document Parser API ì‹œë„
                text, image_paths = extract_text_and_images_from_pdf(pdf_path, pdf_specific_output_dir)
                parser_used = "Upstage Document Parser API"  # íŒŒì„œ ì •ë³´ ì—…ë°ì´íŠ¸
                
                # í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ PyMuPDFë¡œ ëŒ€ì²´
                if not text.strip():
                    log_message("âš ï¸ ê²½ê³ : ì—…ìŠ¤í…Œì´ì§€ APIì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PyMuPDFë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                    text, image_paths = extract_with_pymupdf(pdf_path, pdf_specific_output_dir)
                    parser_used = "PyMuPDF"  # íŒŒì„œ ì •ë³´ ì—…ë°ì´íŠ¸
            except Exception as e:
                log_message(f"âŒ ì˜¤ë¥˜: ì—…ìŠ¤í…Œì´ì§€ API ì‹¤íŒ¨: {str(e)}. PyMuPDFë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                text, image_paths = extract_with_pymupdf(pdf_path, pdf_specific_output_dir)
                parser_used = "PyMuPDF"  # íŒŒì„œ ì •ë³´ ì—…ë°ì´íŠ¸
        
        if not text.strip():
            error_msg = "âŒ ì˜¤ë¥˜: PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            log_message(error_msg)
            return error_msg
        
        log_message(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ì")
        log_message(f"ì¶”ì¶œëœ ì´ë¯¸ì§€ ìˆ˜: {len(image_paths)}ê°œ")
        log_message(f"ì‚¬ìš©ëœ íŒŒì„œ: {parser_used}")
        
        # Generate summary
        log_message(f"\n{language} ì–¸ì–´ë¡œ ìš”ì•½ ìƒì„± ì¤‘... (ëª¨ë¸: {model_type.upper()}{' - ' + model_name if model_name else ''})")
        summary = handler.generate_summary(text, image_paths)
        
        # Print summary to console
        log_message("\n--- Summary ---")
        print(summary)
        log_message("----------------")
        
        # Save summary to file
        summary_path = os.path.join(pdf_specific_output_dir, "summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        log_message(f"\nìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")
        
        # Save model and parser info
        info_path = os.path.join(pdf_specific_output_dir, "process_info.json")
        info = {
            "timestamp": timestamp,
            "pdf_file": os.path.basename(pdf_path),
            "model": {
                "type": model_type.upper(),
                "name": model_name
            },
            "parser": {
                "requested": parser,
                "used": parser_used
            },
            "language": language,
            "stats": {
                "text_length": len(text),
                "image_count": len(image_paths)
            }
        }
        
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        log_message(f"ì²˜ë¦¬ ì •ë³´ ì €ì¥ ì™„ë£Œ: {info_path}")
        
        return summary
        
    
    except Exception as e:
        error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        try:
            log_message(error_msg)
            
            # ì˜¤ë¥˜ ì •ë³´ ì €ì¥
            error_path = os.path.join(pdf_specific_output_dir, "error_log.txt")
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(f"Error Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Error Type: {type(e).__name__}\n")
                f.write(f"Error Message: {str(e)}\n")
                f.write(f"PDF File: {pdf_path}\n")
                f.write(f"Model: {model_type.upper()}{' - ' + model_name if model_name else ''}\n")
                f.write(f"Parser: {parser}\n")
                f.write(f"Language: {language}\n")
            
            log_message(f"ì˜¤ë¥˜ ì •ë³´ ì €ì¥ ì™„ë£Œ: {error_path}")
        except Exception as log_error:
            print(f"ë¡œê·¸ ì €ì¥ ì¤‘ ì¶”ê°€ ì˜¤ë¥˜ ë°œìƒ: {str(log_error)}")
        
        return error_msg

def main():
    parser = argparse.ArgumentParser(description='Process a PDF and generate a summary using different AI models.')
    parser.add_argument('pdf_path', type=str, help='Path to the PDF file to process')
    parser.add_argument('--model', type=str, default='upstage',
                      choices=['upstage', 'llama', 'openai', 'gemini'],
                      help='Model to use for summarization (default: upstage)')
    parser.add_argument('--model-name', type=str, default=None,
                      help='Specific model name/version (e.g., gpt-4-turbo-preview for OpenAI, gemini-pro for Gemini, llama3:latest for Ollama)')
    parser.add_argument('--output-dir', type=str, default='output',
                      help='Directory to save output files (default: output)')
    parser.add_argument('--parser', type=str, default='auto',
                      choices=['auto', 'upstage', 'pymupdf', 'api', 'local'],
                      help='PDF parser to use. Options: auto (try upstage first, fallback to pymupdf), upstage/api (only use upstage API), pymupdf/local (only use PyMuPDF) (default: auto)')
    parser.add_argument('--language', type=str, default='ko',
                      choices=['ko', 'en'],
                      help='Language for the summary (ko: Korean, en: English) (default: ko)')
    
    args = parser.parse_args()
    
    # Set default model names if not provided
    if not args.model_name:
        if args.model == 'openai':
            args.model_name = 'gpt-4-turbo-preview'
        elif args.model == 'gemini':
            args.model_name = 'gemini-pro'
        elif args.model == 'llama':
            args.model_name = 'llama3:latest'
    
    # Process the PDF
    if not os.path.exists(args.pdf_path):
        print(f"âŒ ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.pdf_path}")
        exit(1)
    
    print("=" * 60)
    print(f"ğŸ“„ PDF ë¬¸ì„œ: {os.path.basename(args.pdf_path)}")
    print(f"ğŸ¤– ëª¨ë¸: {args.model.upper()}{' - ' + args.model_name if args.model_name else ''}")
    print(f"ğŸ” íŒŒì„œ: {args.parser}")
    print(f"ğŸŒ ì–¸ì–´: {args.language}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print("=" * 60 + "\n")
    
    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    process_pdf(
        pdf_path=args.pdf_path, 
        model_type=args.model, 
        output_dir=args.output_dir, 
        model_name=args.model_name,
        parser=args.parser,
        language=args.language
    )

if __name__ == "__main__":
    main()
